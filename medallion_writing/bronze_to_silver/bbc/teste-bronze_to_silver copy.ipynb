{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7766777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, udf, pandas_udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import trafilatura\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Inicia SparkSession\n",
    "spark = SparkSession.builder.appName(\"EnriquecimentoSilver\").getOrCreate()\n",
    "\n",
    "# Caminhos\n",
    "bronze_path = \"s3://portfolio-projeto-cinco/datalake/bronze/source=bbc/\"\n",
    "silver_path = \"s3://portfolio-projeto-cinco/datalake/silver/\"\n",
    "\n",
    "# Carrega dados da Bronze\n",
    "df = spark.read.parquet(bronze_path)\n",
    "\n",
    "# Baixa modelo spaCy\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Carrega pipelines HuggingFace\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Fun√ß√£o para extrair texto da not√≠cia\n",
    "@udf(StringType())\n",
    "def extrair_texto_udf(link):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(link)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Extrai texto\n",
    "df = df.withColumn(\"article_text\", extrair_texto_udf(col(\"link\")))\n",
    "\n",
    "# Fun√ß√µes auxiliares em Pandas UDF\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def extrair_entidades_pandas(textos: pd.Series, tipo: str = \"GPE\") -> pd.Series:\n",
    "    return textos.apply(lambda x: list({ent.text for ent in nlp(x).ents if ent.label_ == tipo}) if x else [])\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def resumir_texto_pandas(textos: pd.Series) -> pd.Series:\n",
    "    def resumir(t):\n",
    "        if not t or len(t) < 100:\n",
    "            return \"\"\n",
    "        partes = [t[i:i+1000] for i in range(0, len(t), 1000)]\n",
    "        resumos = []\n",
    "        for parte in partes:\n",
    "            try:\n",
    "                resumo = summarizer(parte, max_length=40, min_length=10, do_sample=False)\n",
    "                resumos.append(resumo[0]['summary_text'])\n",
    "            except:\n",
    "                continue\n",
    "        return \" \".join(resumos)\n",
    "    return textos.apply(resumir)\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def sentimento_pandas(textos: pd.Series) -> pd.Series:\n",
    "    return textos.apply(lambda x: sentiment(x[:512])[0][\"label\"] if x else \"NEUTRAL\")\n",
    "\n",
    "# Aplica enriquecimentos\n",
    "df = df.withColumn(\"locations\", extrair_entidades_pandas(col(\"article_text\")))\n",
    "df = df.withColumn(\"people\", extrair_entidades_pandas(col(\"article_text\"), \"PERSON\"))\n",
    "df = df.withColumn(\"text_summary\", resumir_texto_pandas(col(\"article_text\")))\n",
    "df = df.withColumn(\"overall_feeling\", sentimento_pandas(col(\"article_text\")))\n",
    "\n",
    "# Cria parti√ß√£o de data\n",
    "df = df.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "\n",
    "# Escreve na Silver\n",
    "(\n",
    "    df.write\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"source\", \"date\")\n",
    "    .parquet(silver_path)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dados enriquecidos salvos na camada Silver.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import json\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "# Configura√ß√µes\n",
    "bucket = \"portfolio-projeto-cinco\"\n",
    "log_prefix = \"datalake/logs/bronze_ingest_log/\"\n",
    "bronze_prefix = \"datalake/bronze/\"\n",
    "checkpoint_path = \"datalake-checkpoints/bbc_silver.json\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Carrega o checkpoint da Silver\n",
    "def get_last_silver_checkpoint():\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=checkpoint_path)\n",
    "        data = json.loads(obj['Body'].read().decode(\"utf-8\"))\n",
    "        return datetime.fromisoformat(data[\"last_processed_at\"])\n",
    "    except:\n",
    "        return datetime(2000, 1, 1)\n",
    "\n",
    "# Salva o novo checkpoint\n",
    "def update_silver_checkpoint():\n",
    "    novo_checkpoint = datetime.utcnow().isoformat()\n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=checkpoint_path,\n",
    "        Body=json.dumps({\"last_processed_at\": novo_checkpoint})\n",
    "    )\n",
    "\n",
    "# Lista os arquivos v√°lidos da Bronze usando o log\n",
    "def listar_arquivos_bronze():\n",
    "    last_checkpoint = get_last_silver_checkpoint()\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=log_prefix)\n",
    "\n",
    "    arquivos_validos = []\n",
    "    for page in pages:\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            if not obj[\"Key\"].endswith(\".parquet\"):\n",
    "                continue\n",
    "            buffer = io.BytesIO()\n",
    "            s3.download_fileobj(bucket, obj[\"Key\"], buffer)\n",
    "            buffer.seek(0)\n",
    "            table = pq.read_table(buffer)\n",
    "            df = table.to_pandas()\n",
    "            \n",
    "            print(f\"{len(df)} atualiza√ß√µes encontradas\")\n",
    "\n",
    "            df_filtrado = df[\n",
    "                (df[\"source\"] == \"bbc\") &\n",
    "                (df[\"status\"] == \"success\") &\n",
    "                (pd.to_datetime(df[\"processed_at\"]) > last_checkpoint)\n",
    "            ]\n",
    "            arquivos_validos.extend(df_filtrado[\"path\"].tolist())\n",
    "\n",
    "    return arquivos_validos\n",
    "\n",
    "# Exemplo de leitura dos arquivos da bronze filtrados\n",
    "def carregar_dados_bronze():\n",
    "    arquivos = listar_arquivos_bronze()\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum novo arquivo para processar.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dfs = []\n",
    "    for path in arquivos:\n",
    "        buffer = io.BytesIO()\n",
    "        s3.download_fileobj(bucket, path.replace(f\"s3://{bucket}/\", \"\"), buffer)\n",
    "        buffer.seek(0)\n",
    "        table = pq.read_table(buffer)\n",
    "        dfs.append(table.to_pandas())\n",
    "\n",
    "    df_bronze = pd.concat(dfs, ignore_index=True)\n",
    "    return df_bronze\n",
    "\n",
    "# Execu√ß√£o principal\n",
    "df = carregar_dados_bronze()\n",
    "if not df.empty:\n",
    "    print(f\"üîç {len(df)} registros carregados da Bronze.\")\n",
    "    update_silver_checkpoint()\n",
    "    print(\"Checkpoint atualizado.\")\n",
    "else:\n",
    "    print(\"Nenhum dado carregado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce610c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trafilatura spacy transformers keybert; python -m spacy download en_core_web_sm\n",
    "import trafilatura\n",
    "from keybert import KeyBERT\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "# Supress√£o de warnings de parsing XML\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Carregamento de modelos\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def extrair_texto(link):\n",
    "    try:\n",
    "        baixado = trafilatura.fetch_url(link)\n",
    "        if baixado:\n",
    "            return trafilatura.extract(baixado, include_comments=False, include_tables=False)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair texto de {link}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extrair_entidades(texto, tipo):\n",
    "    try:\n",
    "        doc = nlp(texto)\n",
    "        return list(set(ent.text for ent in doc.ents if ent.label_ == tipo))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def resumir_texto(texto, max_chunk_chars=1000):\n",
    "    if not texto or len(texto) < 100:\n",
    "        return \"\"\n",
    "    partes = [texto[i:i+max_chunk_chars] for i in range(0, len(texto), max_chunk_chars)]\n",
    "    resumos = []\n",
    "    for parte in partes:\n",
    "        try:\n",
    "            out = summarizer(parte, max_length=40, min_length=10, do_sample=False)\n",
    "            resumos.append(out[0][\"summary_text\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \" \".join(resumos)\n",
    "\n",
    "def sentimento_principal(texto):\n",
    "    try:\n",
    "        return sentiment(texto[:512])[0][\"label\"]\n",
    "    except Exception:\n",
    "        return \"NEUTRAL\"\n",
    "\n",
    "def enriquecer_dataframe(df):\n",
    "    df[\"article_text\"] = df[\"link\"].apply(extrair_texto)\n",
    "    df[\"people\"] = df[\"article_text\"].apply(lambda x: extrair_entidades(x, \"PERSON\"))\n",
    "    df[\"locations\"] = df[\"article_text\"].apply(lambda x: extrair_entidades(x, \"GPE\"))\n",
    "    df[\"text_summary\"] = df[\"article_text\"].apply(resumir_texto)\n",
    "    df[\"overall_feeling\"] = df[\"article_text\"].apply(sentimento_principal)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_enriquecido = enriquecer_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04375053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6017fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import json\n",
    "from datetime import datetime\n",
    "import io\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import os\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# Configura√ß√µes\n",
    "bucket = \"portfolio-projeto-cinco\"\n",
    "log_prefix = \"datalake/logs/bronze_ingest_log/\"\n",
    "bronze_prefix = \"datalake/bronze/\"\n",
    "checkpoint_path = \"datalake-checkpoints/bbc_silver.json\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Carrega o checkpoint da Silver\n",
    "def get_last_silver_checkpoint():\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=checkpoint_path)\n",
    "        data = json.loads(obj['Body'].read().decode(\"utf-8\"))\n",
    "        return datetime.fromisoformat(data[\"last_processed_at\"])\n",
    "    except:\n",
    "        return datetime.min\n",
    "\n",
    "# Salva o novo checkpoint\n",
    "def update_silver_checkpoint():\n",
    "    novo_checkpoint = datetime.utcnow().isoformat()\n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=checkpoint_path,\n",
    "        Body=json.dumps({\"last_processed_at\": novo_checkpoint})\n",
    "    )\n",
    "\n",
    "# Lista os arquivos v√°lidos da Bronze usando o log\n",
    "def listar_arquivos_bronze():\n",
    "    last_checkpoint = get_last_silver_checkpoint()\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=log_prefix)\n",
    "\n",
    "    arquivos_validos = []\n",
    "    for page in pages:\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            if not obj[\"Key\"].endswith(\".parquet\"):\n",
    "                continue\n",
    "            buffer = io.BytesIO()\n",
    "            s3.download_fileobj(bucket, obj[\"Key\"], buffer)\n",
    "            buffer.seek(0)\n",
    "            table = pq.read_table(buffer)\n",
    "            df = table.to_pandas()\n",
    "\n",
    "            df_filtrado = df[\n",
    "                (df[\"source\"] == \"bbc\") &\n",
    "                (df[\"status\"] == \"success\") &\n",
    "                (pd.to_datetime(df[\"processed_at_ts\"]) > last_checkpoint)\n",
    "            ]\n",
    "            arquivos_validos.extend(df_filtrado[\"path\"].tolist())\n",
    "\n",
    "    return arquivos_validos\n",
    "\n",
    "# Exemplo de leitura dos arquivos da bronze filtrados\n",
    "def carregar_dados_bronze():\n",
    "    arquivos = listar_arquivos_bronze()\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum novo arquivo para processar.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dfs = []\n",
    "    for path in arquivos:\n",
    "        buffer = io.BytesIO()\n",
    "        s3.download_fileobj(bucket, path.replace(f\"s3://{bucket}/\", \"\"), buffer)\n",
    "        buffer.seek(0)\n",
    "        table = pq.read_table(buffer)\n",
    "        dfs.append(table.to_pandas())\n",
    "\n",
    "    df_bronze = pd.concat(dfs, ignore_index=True)\n",
    "    return df_bronze\n",
    "\n",
    "# Execu√ß√£o principal\n",
    "df = carregar_dados_bronze()\n",
    "if not df.empty:\n",
    "    print(f\"üîç {len(df)} registros carregados da Bronze.\")\n",
    "    update_silver_checkpoint()\n",
    "    print(\"‚úÖ Checkpoint atualizado.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado carregado.\")\n",
    "    \n",
    "prompt_template = \"\"\"\n",
    "A partir do texto abaixo, extraia as seguintes informa√ß√µes:\n",
    "- Um resumo em uma frase.\n",
    "- O sentimento principal entre: POSITIVO, NEGATIVO ou NEUTRO.\n",
    "- Lista de locais mencionados, incluindo pa√≠ses, estados e cidades.\n",
    "- Lista de pessoas mencionadas.\n",
    "\n",
    "Texto:\n",
    "\\\"\\\"\\\"\n",
    "{texto}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Retorne no seguinte formato JSON:\n",
    "{{\n",
    "  \"resumo\": \"...\",\n",
    "  \"sentimento\": \"...\",\n",
    "  \"locais\": [...],\n",
    "  \"pessoas\": [...]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def extrair_texto(link):\n",
    "    try:\n",
    "        baixado = trafilatura.fetch_url(link)\n",
    "        if baixado:\n",
    "            return trafilatura.extract(baixado, include_comments=False, include_tables=False)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair texto de {link}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def enriquecer_artigo(texto):\n",
    "    prompt = prompt_template.format(texto=texto)  # corta para economizar tokens\n",
    "    client = openai.OpenAI(api_key=os.getenv('TOKEN_OPENAI'))\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini-2025-04-14\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        result = (\n",
    "            response\n",
    "            .choices[0]\n",
    "            .message\n",
    "            .content\n",
    "            .replace(\"```\", '')\n",
    "            .replace('json', '')\n",
    "        )\n",
    "        \n",
    "        return json.loads(result)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no enriquecimento: {e}\")\n",
    "        return {\n",
    "            \"resumo\": None,\n",
    "            \"sentimento\": None,\n",
    "            \"locais\": [],\n",
    "            \"pessoas\": []\n",
    "        }\n",
    "\n",
    "df[\"article_text\"] = df[\"link\"].apply(extrair_texto)\n",
    "df['genai_output'] = df[\"article_text\"].apply(enriquecer_artigo)\n",
    "df_exploded = pd.json_normalize(df[\"genai_output\"])\n",
    "df_final = pd.concat([df.drop(columns=[\"genai_output\"]), df_exploded], axis=1)\n",
    "\n",
    "# Defini√ß√µes de timestamp e source do dataframe\n",
    "df_final[\"processed_at\"] = datetime.utcnow()\n",
    "df_final[\"date\"] = df_final[\"processed_at\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df_final[\"source\"] = \"bbc\"  # garantir compatibilidade\n",
    "\n",
    "for date, group in df_final.groupby(\"date\"):\n",
    "    table = pa.Table.from_pandas(group)\n",
    "    buffer = BytesIO()\n",
    "    pq.write_table(table, buffer)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    filename = f\"bbc_silver_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.parquet\"\n",
    "    target_key = f\"datalake/silver/source=bbc/date={date}/{filename}\"\n",
    "    \n",
    "    s3.upload_fileobj(buffer, Bucket=bucket, Key=target_key)\n",
    "    print(f\"‚úÖ Arquivo escrito em: s3://{bucket}/{target_key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdfe5fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_texto(link):\n",
    "    try:\n",
    "        baixado = trafilatura.fetch_url(link)\n",
    "        if baixado:\n",
    "            return trafilatura.extract(baixado, include_comments=False, include_tables=False)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair texto de {link}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "df_teste = pd.read_parquet('/home/omarcocaja/Desktop/portfolio/projeto_5_webscraping/dev-s3/portfolio-projeto-cinco/datalake/bronze/source=bbc/date=2025-06-12/bbc_2025_06_12_10_42_08_downtown_la_under_curfew_for_second_night_after_days_of_protests.parquet')\n",
    "\n",
    "df_teste[\"article_text\"] = df_teste[\"link\"].apply(extrair_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "350b5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste['genai_output'] = df_teste[\"article_text\"].apply(enriquecer_artigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4af1c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = pd.json_normalize(df_teste[\"genai_output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "258bff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_teste.drop(columns=[\"genai_output\"]), df_exploded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09c07075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>guid</th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>article_text</th>\n",
       "      <th>resumo</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>locais</th>\n",
       "      <th>pessoas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Downtown LA under curfew for second night afte...</td>\n",
       "      <td>LA Mayor Karen Bass has accused federal author...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cn7z45pyrvvo</td>\n",
       "      <td>Thu, 12 Jun 2025 07:09:16 GMT</td>\n",
       "      <td>https://www.bbc.com/news/articles/cn7z45pyrvvo#0</td>\n",
       "      <td>bbc</td>\n",
       "      <td>2025-06-12 07:09:16+00:00</td>\n",
       "      <td>2025-06-12</td>\n",
       "      <td>Downtown LA under curfew for second night afte...</td>\n",
       "      <td>Los Angeles enfrenta toque de recolher e forte...</td>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>[Los Angeles, Downtown LA, Estados Unidos, Cal...</td>\n",
       "      <td>[Karen Bass, Donald Trump, Jim McDonnell, Pam ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Downtown LA under curfew for second night afte...   \n",
       "\n",
       "                                         description  \\\n",
       "0  LA Mayor Karen Bass has accused federal author...   \n",
       "\n",
       "                                             link  \\\n",
       "0  https://www.bbc.com/news/articles/cn7z45pyrvvo   \n",
       "\n",
       "                         pubDate  \\\n",
       "0  Thu, 12 Jun 2025 07:09:16 GMT   \n",
       "\n",
       "                                               guid source  \\\n",
       "0  https://www.bbc.com/news/articles/cn7z45pyrvvo#0    bbc   \n",
       "\n",
       "                  timestamp        date  \\\n",
       "0 2025-06-12 07:09:16+00:00  2025-06-12   \n",
       "\n",
       "                                        article_text  \\\n",
       "0  Downtown LA under curfew for second night afte...   \n",
       "\n",
       "                                              resumo sentimento  \\\n",
       "0  Los Angeles enfrenta toque de recolher e forte...   NEGATIVO   \n",
       "\n",
       "                                              locais  \\\n",
       "0  [Los Angeles, Downtown LA, Estados Unidos, Cal...   \n",
       "\n",
       "                                             pessoas  \n",
       "0  [Karen Bass, Donald Trump, Jim McDonnell, Pam ...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
