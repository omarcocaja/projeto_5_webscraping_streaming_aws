# Pipeline de Webscraping em Streaming com AWS, Kafka e Spark

Este projeto implementa um pipeline de dados **em tempo real** que consome notÃ­cias e eventos de diferentes fontes via **webscraping**, publica os dados em tÃ³picos Kafka e os armazena em uma arquitetura de **Data Lake** baseada na AWS, com estrutura de camadas **Bronze** e futura evoluÃ§Ã£o para Silver/Gold.

O objetivo Ã© simular um pipeline de ingestÃ£o contÃ­nua de dados nÃ£o estruturados em um cenÃ¡rio de **monitoramento de notÃ­cias em tempo real**.

---

## ðŸ—‚ Estrutura do Projeto

```
projeto_5_webscraping_streaming_aws/
â”‚
â”œâ”€â”€ kafka_files/
â”‚   â”œâ”€â”€ kafka_utils.py                   # FunÃ§Ãµes de utilidade para envio de mensagens ao Kafka
â”‚
â”œâ”€â”€ kafka_consumers/
â”‚   â””â”€â”€ emsc_consumer_to_s3.py          # Spark Consumer que escreve os dados da bronze no S3
â”‚
â”œâ”€â”€ web_sources/
â”‚   â”œâ”€â”€ BBC/                            # Scraper para a BBC News (RSS/XML)
â”‚   â”œâ”€â”€ ReliefWeb/                      # Scraper para atualizaÃ§Ãµes humanitÃ¡rias globais
â”‚   â””â”€â”€ Reddit/                         # Scraper para subreddits via RSS
â”‚
â”œâ”€â”€ 2 - Kafka/
â”‚   â””â”€â”€ write_to_kafka.py               # FunÃ§Ã£o comum para todos os crawlers publicarem em Kafka
â”‚
â”œâ”€â”€ dev-s3/                             # Estrutura local simulando o bucket S3 (data lake)
â”‚   â””â”€â”€ teste-portfolio-projeto5/
â”‚       â””â”€â”€ datalake/
â”‚           â””â”€â”€ bronze/                 # Camada bronze com arquivos particionados por data e fonte
â”‚
â”œâ”€â”€ docker-compose.yaml                 # Sobe Kafka, Zookeeper e demais serviÃ§os de infraestrutura
â”œâ”€â”€ requirements.txt                    # DependÃªncias Python
â”œâ”€â”€ consume_topic.sh                    # Script de debug para consumir dados de um tÃ³pico Kafka
â””â”€â”€ README.md                           # Este arquivo
```

---

## âš™ï¸ Como Executar

### 1. Suba os serviÃ§os com Docker
```bash
docker-compose up -d
```

### 2. Execute os crawlers (localmente)
```bash
python web_sources/BBC/bbc_scraper.py
```

> Os dados sÃ£o publicados em tÃ³picos Kafka automaticamente com a funÃ§Ã£o `write_to_kafka`.

### 3. Execute o consumidor Spark
```bash
spark-submit kafka_consumers/emsc_consumer_to_s3.py
```

> O consumer escreve os dados da camada bronze em arquivos `.parquet` no S3 simulado (`dev-s3/`).

---

## ðŸ“¦ Tecnologias Utilizadas

- **Kafka + Zookeeper**
- **Apache Spark Structured Streaming**
- **Python (Playwright, feedparser, boto3)**
- **Docker e Docker Compose**
- **AWS S3 (simulado localmente)**
- **AWS**
- **Formato Parquet**

---

## ðŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a **MIT** e pode ser livremente utilizado e adaptado.

---

## ðŸ“¬ Contato

- [LinkedIn](https://www.linkedin.com/in/marco-caja)  
- [Instagram](https://www.instagram.com/omarcocaja)
